---
title: Consistency
sidebar_position: 4
image: og/docs/concepts.jpg
# tags: ['architecture']
---

import SkipLink from '/src/components/SkipValidationLink'

Weaviate における レプリケーション ファクター は、シャード（レプリカ とも呼ばれます）のコピーを Weaviate クラスター全体にいくつ保持するかを決定します。

<p align="center"><img src="/img/docs/replication-architecture/replication-factor.png" alt="Replication factor" width="80%"/></p>

レプリケーション ファクター が  >  1 の場合、コンシステンシー モデルによってシステムの信頼性、スケーラビリティ、および / またはパフォーマンス要件のバランスを取ります。

Weaviate は複数のコンシステンシー モデルを使用します。クラスター メタデータ用とデータ オブジェクト用の 2 種類です。

### Weaviate における一貫性モデル

Weaviate は [Raft](https://raft.github.io/) コンセンサス アルゴリズムを使用して [クラスター メタデータのレプリケーション](./cluster-architecture.md#metadata-replication-raft) を行います。ここでいうクラスター メタデータには、コレクション定義やテナントの稼働状況が含まれます。これにより、いくつかのノードがダウンしていてもクラスター メタデータを更新できます。

データ オブジェクトは、チューナブルなコンシステンシー レベルを持つ [リーダーレス設計](./cluster-architecture.md#data-replication-leaderless) でレプリケートされます。そのため、データ操作は必要に応じて一貫性重視にも可用性重視にも調整できます。

これらの設計は、[CAP 定理](./index.md#cap-theorem) に記載されている一貫性と可用性のトレードオフを反映しています。

:::tip Rule of thumb on consistency
一貫性の強さは以下の条件で判断できます。  
* もし  r + w > n  なら、システムは強い一貫性を持ちます。  
    *  r は読み取り操作のコンシステンシー レベル  
    *  w は書き込み操作のコンシステンシー レベル  
    *  n はレプリケーション ファクター（レプリカ数）  
* もし  r + w &lt;= n  なら、このシナリオで到達可能なのは最終的 (eventual) 一貫性が最大です。
:::

## クラスター メタデータ

Weaviate のクラスター メタデータは Raft アルゴリズムを使用します。

` v1.25 ` から、Weaviate はクラスター メタデータのレプリケーションに [Raft](https://raft.github.io/) コンセンサス アルゴリズムを採用しています。Raft はリーダー ノードを選出し、ログ ベースの方式でクラスター全体にレプリケーションを調整するアルゴリズムです。

その結果、クラスター メタデータを変更するリクエストは必ずリーダー ノードに送信されます。リーダー ノードは自身のログに変更を適用し、その後フォロワー ノードへ変更を伝播します。クラスターの過半数 (quorum) のノードがメタデータ変更を承認すると、リーダー ノードは変更をコミットし、クライアントへ応答を返します。

このアーキテクチャにより、少数のノードが障害を起こしてもクラスター メタデータは一貫して維持されます。

<details>
  <summary>Pre-<code>v1.25</code> クラスター メタデータのコンセンサス アルゴリズム</summary>

Raft 導入以前、クラスター メタデータの更新は [分散トランザクション](https://en.wikipedia.org/wiki/Distributed_transaction) アルゴリズムで実施されていました。これは分散ネットワーク上の複数ノードにあるデータベース間で一連の操作を行う方法です。Weaviate は [2 フェーズ コミット (2PC)](https://en.wikipedia.org/wiki/Two-phase_commit_protocol) プロトコルを使用し、ミリ秒単位でクラスター メタデータをレプリケートしていました。

障害がない場合の実行は  2  段階です。  
1. コミット要求フェーズ（投票フェーズ）：コーディネーター ノードが各ノードに更新を受信・処理できるか確認します。  
2. コミット フェーズ：コーディネーターが変更をノードへコミットします。

</details>

### クエリにおけるコレクション定義の取得

:::info Added in ` v1.27.10 `, ` v1.28.4 `
:::

一部のクエリではコレクション定義が必要です。本機能導入前は、そのようなクエリごとにローカル（リクエスト発行）ノードがリーダー ノードからコレクション定義を取得していました。この方法では定義は強い一貫性を保ちますが、追加のトラフィックと負荷を生じる可能性があります。

利用可能な場合、`COLLECTION_RETRIEVAL_STRATEGY` [環境変数](/deploy/configuration/env-vars/index.md#multi-node-instances) を `LeaderOnly`、`LocalOnly`、`LeaderOnMismatch` に設定できます。

- `LeaderOnly`（デフォルト）：常にリーダー ノードから定義を取得します。最も一貫性が高い一方、クラスタ内トラフィックが増える可能性があります。
- `LocalOnly`：常にローカルの定義を使用します。最終的 (eventual) 一貫性となりますが、クラスタ内トラフィックを削減できます。
- `LeaderOnMismatch`：ローカル定義が古いかを確認し、必要に応じてリーダー ノードから定義を取得します。一貫性とトラフィックのバランスを取ります。

デフォルトは `LeaderOnly` で強い一貫性を確保します。ただし、必要な一貫性レベルに応じて `LocalOnly` や `LeaderOnMismatch` を使用し、クラスタ内トラフィックを削減できます。

## データ オブジェクト

Weaviate はオブジェクトに対して 2 フェーズ コミットを、コンシステンシー レベルに応じて調整しながら使用します。たとえば `QUORUM` 書き込み（後述）の場合、ノードが  5  台なら  3  件のリクエストを送信し、それぞれが内部的に 2 フェーズ コミットを実行します。

その結果、Weaviate のデータ オブジェクトは最終的 (eventual) に一貫性が取れます。最終的一貫性は BASE セマンティクスを提供します。

* **Basically available**: 読み書きは可能な限り利用可能  
* **Soft-state**: 更新がまだ収束していないため一貫性保証はありません  
* **Eventually consistent**: システムが十分長く稼働すれば、書き込み後に全ノードが一貫します  

Weaviate は可用性向上のために最終的一貫性を使用します。読み取り・書き込みの一貫性は調整可能で、アプリケーションのニーズに合わせて可用性と一貫性のトレードオフを取れます。

*下のアニメーションは、レプリケーション ファクター  3 、ノード  8  台の環境で Weaviate が書き込みまたは読み取りを行う例です。青色のノードがコーディネーター ノードとして動作します。コンシステンシー レベルは `QUORUM` に設定されているため、コーディネーター ノードはクライアントへ結果を返す前に  3  件中  2  件の応答のみを待ちます。*

<p align="center"><img src="/img/docs/replication-architecture/replication-quorum-animation.gif" alt="Write consistency QUORUM" width="75%"/></p>

### 調整可能な書き込み一貫性

データ オブジェクトの追加や変更は **書き込み** 操作です。

:::note
書き込み操作は Weaviate  1.18 以降で `ONE`、`QUORUM`（デフォルト）、`ALL` に設定できます。  v1.17  では書き込み操作は常に `ALL`（最も高い一貫性）でした。
:::

書き込み一貫性を設定可能にした主な理由は、  v1.18  で自動修復機能が導入されたためです。書き込みは選択したコンシステンシー レベルに関係なく常に  n  （レプリケーション ファクター）ノードへ書き込まれます。ただし、コーディネーター ノードが返答を待つノード数は `ONE`、`QUORUM`、`ALL` により異なります。読み取り時の修復機能がない状態で書き込みを確実に全ノードへ適用するため、  v1.17  までは書き込み一貫性が `ALL` に固定されていました。  v1.18+ では以下を設定できます。  
* **ONE** - 少なくとも  1  つのレプリカ ノードから ACK を受け取れば書き込み成功とみなします。最速（最も可用性が高い）ですが一貫性は最も低くなります。  
* **QUORUM** - `QUORUM` 数のレプリカ ノードから ACK を受け取る必要があります。`QUORUM` は _n / 2 + 1_ で計算され、_n_ はレプリケーション ファクターです。たとえば レプリケーション ファクター  6  の場合、クォーラムは  4  であり、  2  レプリカまでダウンを許容できます。  
* **ALL** - すべてのレプリカ ノードから ACK を受け取る必要があります。最も一貫性が高い一方、最も「遅い」（可用性が低い）選択肢です。  


*下図: レプリケーション ファクター  3 、ノード  8  台で、書き込み一貫性 `ONE` の Weaviate 構成。*

<p align="center"><img src="/img/docs/replication-architecture/replication-rf3-c-ONE.png" alt="Write consistency ONE" width="60%"/></p>

*下図: レプリケーション ファクター  3 、ノード  8  台で、書き込み一貫性 `QUORUM`（ n/2 + 1 ）の Weaviate 構成。*

<p align="center"><img src="/img/docs/replication-architecture/replication-rf3-c-QUORUM.png" alt="Write consistency QUORUM" width="60%"/></p>

*下図: レプリケーション ファクター  3 、ノード  8  台で、書き込み一貫性 `ALL` の Weaviate 構成。*

<p align="center"><img src="/img/docs/replication-architecture/replication-rf3-c-ALL.png" alt="Write consistency ALL" width="60%"/></p>


### 調整可能な読み取り一貫性

読み取り操作は、Weaviate のデータ オブジェクトに対する GET リクエストです。書き込みと同様に、読み取り一貫性も `ONE`、`QUORUM`（デフォルト）、`ALL` に設定できます。

:::note
` v1.18 ` 以前は、一貫性レベルを設定できたのは [ID でオブジェクトを取得するリクエスト](../../manage-objects/read.mdx#get-an-object-by-id) のみで、他のすべての読み取りリクエストは一貫性 `ALL` でした。
:::

以下の一貫性レベルはほとんどの読み取り操作に適用されます。  

- ` v1.18 ` から、REST エンドポイント操作に適用されます。  
- ` v1.19 ` から、GraphQL `Get` リクエストに適用されます。  
- すべての gRPC ベースの読み取り・書き込み操作はチューナブルな一貫性レベルをサポートします。  

* **ONE** - 少なくとも  1  つのレプリカから応答が返れば読み取り成功とみなします。最速（最も可用性が高い）ですが一貫性は最も低くなります。  
* **QUORUM** - `QUORUM` 数のレプリカ ノードから応答が返る必要があります。`QUORUM` は _n / 2 + 1_ で計算されます。たとえば レプリケーション ファクター  6  の場合、クォーラムは  4  であり、  2  レプリカまでダウンを許容できます。  
* **ALL** - すべてのレプリカから応答が返らない場合、読み取りが失敗します。最も一貫性が高い一方、最も「遅い」（可用性が低い）選択肢です。  

例:  
* **ONE**<br/>
  単一データセンターでレプリケーション ファクター  3 、読み取り一貫性 `ONE` の場合、コーディネーター ノードは  1  レプリカ ノードからの応答を待ちます。

  <p align="center"><img src="/img/docs/replication-architecture/replication-rf3-c-ONE.png" alt="Write consistency ONE" width="60%"/></p>

* **QUORUM**<br/>
  単一データセンターでレプリケーション ファクター  3 、読み取り一貫性 `QUORUM` の場合、コーディネーター ノードは  n / 2 + 1 = 3 / 2 + 1 = 2  レプリカ ノードからの応答を待ちます。

  <p align="center"><img src="/img/docs/replication-architecture/replication-rf3-c-QUORUM.png" alt="Write consistency QUORUM" width="60%"/></p>

* **ALL**<br/>
  単一データセンターでレプリケーション ファクター  3 、読み取り一貫性 `ALL` の場合、コーディネーター ノードは  3  つすべてのレプリカ ノードからの応答を待ちます。

  <p align="center"><img src="/img/docs/replication-architecture/replication-rf3-c-ALL.png" alt="Write consistency ALL" width="60%"/></p>
### 調整可能な整合性戦略

整合性と速度のトレードオフに応じて、以下は書き込み / 読み取り操作における 3 つの一般的な整合性レベルの組み合わせです。これらは最終的に一貫したデータを保証する _最小_ 要件です:
* `QUORUM` / `QUORUM` => 書き込みと読み取りのレイテンシを均衡
* `ONE` / `ALL` => 高速な書き込みと低速な読み取り（書き込み重視）
* `ALL` / `ONE` => 低速な書き込みと高速な読み取り（読み取り重視）

### 調整可能な整合性とクエリ

読み取り操作の調整可能な整合性レベルは、クエリによって返されるオブジェクト一覧の整合性には影響しません。言い換えると、クエリで返されるオブジェクト UUID の一覧は、コーディネーターノード（および必要なシャード）のローカルインデックスのみに依存し、読み取り整合性レベルとは無関係です。

これは、各クエリがコーディネーターノードとクエリを解決するために必要な他のシャードによって実行されるためです。たとえ読み取り整合性レベルが `ALL` に設定されていても、複数のレプリカにクエリを送り結果をマージするわけではありません。

読み取り整合性レベルが適用されるのは、同定されたオブジェクトをレプリカから取得する際です。たとえば、読み取り整合性レベルが `ALL` に設定されている場合、コーディネーターノードはすべてのレプリカがオブジェクトを返すまで待機します。`ONE` の場合、コーディネーターノードは自ノードのオブジェクトのみを返すことがあります。

つまり、読み取り整合性レベルは取得されるオブジェクトのバージョンにのみ影響し、クエリ結果をより（またはそれほど）一貫性のあるものにするわけではありません。

:::note When might this occur?

デフォルトでは、 Weaviate は挿入 / 更新 / 削除時にすべてのノードへ書き込みを行います。そのため、ほとんどの場合、すべてのシャードは互いに同一のローカルインデックスを保持しており、この問題は発生しません。これはノードがダウンしている、あるいはネットワーク障害があるなど、問題が発生したときにまれに起こるケースです。

:::

### テナント状態とデータオブジェクト

[マルチテナントコレクション](../data.md#multi-tenancy) の各テナントには、テナントのデータの可用性と配置を決定する [テナント状態](../../starter-guides/managing-resources/tenant-states.mdx) を設定できます。テナント状態は `active`、`inactive`、`offloaded` に設定可能です。

`active` テナントのデータはクエリや更新が可能であるべきですが、`inactive` や `offloaded` テナントではそうではありません。

ただし、テナント状態を変更してから実際にデータがその（宣言的な）状態を反映するまでに遅延が生じる場合があります。

その結果、テナント状態が `inactive` または `offloaded` に設定されていても一定期間クエリ可能な場合があります。逆に、テナント状態が `active` に設定されていても一定期間クエリや更新ができない場合があります。

:::info Why is this not addressed by repair-on-read?
速度を優先するため、テナントに対するデータ操作はテナントのアクティブ状態操作とは独立して行われます。そのため、テナント状態は repair-on-read 操作によって更新されません。
:::

## 修復

Weaviate のような分散システムでは、ネットワーク問題、ノード障害、タイミング競合など様々な理由でオブジェクトのレプリカが不整合になることがあります。 Weaviate はレプリカ間で不整合なデータを検出すると、同期していないデータの修復を試みます。

Weaviate は [非同期レプリケーション](#async-replication)、[削除解決](#deletion-resolution-strategies)、[repair-on-read](#repair-on-read) 戦略を用いてレプリカ間の整合性を維持します。

### 非同期レプリケーション

:::info Added in `v1.26`
:::

非同期レプリケーションは、同じデータを保持するノード間で最終的な整合性を確保する Weaviate のバックグラウンド同期プロセスです。各シャードが複数ノードに複製されている場合、非同期レプリケーションはハッシュツリー（Merkle ツリー）アルゴリズムを用いてノード間の状態を定期的に比較し、データを伝播することで、同じデータを保持するすべてのノードが同期した状態を保ちます。

アルゴリズムが不整合を検出すると、そのノードのデータを再同期します。

repair-on-read は 1 つまたは 2 つの孤立した修復には適していますが、多数の不整合がある状況では非同期レプリケーションが有効です。たとえば、オフラインになったノードが一連の更新を逃した場合、ノードがサービスに復帰すると非同期レプリケーションがすばやく整合性を回復します。

非同期レプリケーションは repair-on-read を補完します。同期チェックの間にノードが不整合になった場合、repair-on-read が読み取り時に問題を検出します。

非同期レプリケーションを有効化するには、[コレクション定義の](../../manage-collections/multi-node-setup.mdx#replication-settings) `replicationConfig` セクションで `asyncEnabled` を true に設定してください。利用可能な設定については [How-to: Replication](/deploy/configuration/replication.md#async-replication-settings) を参照してください。

#### 非同期レプリケーションのメモリとパフォーマンスの考慮事項

:::info Added in `v1.29`
:::

非同期レプリケーションは、オブジェクトの最終更新時間に基づき、クラスターノード間のデータを比較・同期するためにハッシュツリーを使用します。このプロセスで必要となる追加メモリは、ハッシュツリーの高さ (`H`) によって決まります。ツリーが高いほどメモリ使用量は増えますが、ハッシュが速くなり、不整合の検出と修復に要する時間が短縮されます。

トレードオフは以下のとおりです:
  - **高い** `H`: メモリ使用量が多い、レプリケーションが速い。
  - **低い** `H`: メモリ使用量が少ない、レプリケーションが遅い。

:::tip マルチテナンシーにおけるメモリ管理
各テナントはシャードに対応しています。そのため、多数のテナントがある場合、非同期レプリケーションのメモリ消費が大きくなる可能性があります。（例: テナント 1,000 件でハッシュツリーの高さが 16 の場合、ノードあたり追加で約 2 GB、20 の場合は約 34 GB が必要）
<br/>

メモリ消費を抑えるにはハッシュツリーの高さを下げてください。ただし、ハッシュが遅くなり、レプリケーション速度が低下する可能性があります。
:::

以下の式と例をクイックリファレンスとしてご利用ください:

##### メモリ計算

- **ハッシュツリーのノード総数:**
  高さ `H` のハッシュツリーのノード総数は次のとおりです:
  ```
  Number of hash tree nodes = 2^(H+1) - 1 ≈ 2^(H+1)
  ```

- **必要メモリ合計（各ノードのシャード / テナントあたり）:**
  各ハッシュツリーノードは約 **16 bytes** のメモリを使用します。
  ```
  Memory Required ≈ 2^(H+1) * 16 bytes
  ```

##### 例

- 高さ `16` のハッシュツリー:
  - `Total hash tree nodes ≈ 2^(16+1) = 131,072`
  - `Memory required ≈ 131072 * 16 bytes ≈ 2,097,152 bytes (~2 MB)`

- 高さ `20` のハッシュツリー:
  - `Total hash tree nodes ≈ 2^(20+1) = 2,097,152`
  - `Memory required ≈ 2,097,152 * 16 bytes ≈ 33,554,432 bytes (~33 MB)`

##### パフォーマンス考慮: 葉ノード数

シャード（例: テナント）内のオブジェクトはハッシュツリーの葉に分散されます。ハッシュツリーが大きいほど、各葉がハッシュするデータ量が少なくなるため、比較が速くなりレプリケーションも速くなります。

- **ハッシュツリーの葉ノード数:**
  ```
  Number of leaves = 2^H
  ```

##### 例

- 高さ `16` のハッシュツリー:
  - `Number of Leaves = 2^16 = 65,536`

- 高さ `20` のハッシュツリー:
  - `Number of Leaves = 2^20 = 1,048,576`

:::note デフォルト設定
デフォルトのハッシュツリー高さ `16` は、メモリ消費とレプリケーション性能のバランスを取るために選択されています。クラスターノードの利用可能リソースと性能要件に応じて調整してください。
:::

### 削除解決戦略

:::info Added in `v1.28`
:::

オブジェクトが一部のレプリカに存在し、他のレプリカに存在しない場合、それは作成がまだすべてのレプリカに伝播されていないか、削除がまだ伝播されていないかのいずれかです。この 2 つを区別することが重要です。

削除解決は、非同期レプリケーションおよび repair-on-read と連携し、クラスター全体で削除されたオブジェクトを一貫して処理します。各コレクションごとに、以下のいずれかの削除解決戦略を設定できます(../../manage-collections/multi-node-setup.mdx#replication-settings):

- `NoAutomatedResolution`
- `DeleteOnConflict`
- `TimeBasedResolution`

削除解決戦略は変更可能です。[コレクション定義の更新方法](../../manage-collections/collection-operations.mdx#update-a-collection-definition) を参照してください。
#### `NoAutomatedResolution`

これはデフォルト設定であり、 `v1.28` より前の Weaviate バージョンでは唯一の設定です。このモードでは Weaviate は削除競合を特別なケースとして扱いません。オブジェクトが一部のレプリカに存在し、他のレプリカには存在しない場合、 Weaviate は欠落しているレプリカにオブジェクトを復元する可能性があります。

#### `DeleteOnConflict`

`deleteOnConflict` での削除競合は、常にすべてのレプリカからオブジェクトを削除することで解決されます。

そのため、 Weaviate は削除リクエストを受け取った際、オブジェクトの痕跡を完全に削除するのではなく、各レプリカ上のオブジェクトを「削除済みオブジェクト」として更新します。

#### `TimeBasedResolution`

`timeBasedResolution` での削除競合は、削除リクエストのタイムスタンプを、その後の作成や更新などのオブジェクト操作のタイムスタンプと比較して解決します。

削除リクエストのタイムスタンプが後の場合、オブジェクトはすべてのレプリカで削除されます。削除リクエストのタイムスタンプが前の場合、後から行われた更新がすべてのレプリカに適用されます。

例えば:
- オブジェクトが timestamp 100 で削除され、その後 timestamp 90 で再作成された場合、再作成が優先されます  
- オブジェクトが timestamp 100 で削除され、その後 timestamp 110 で再作成された場合、削除が優先されます  

#### 戦略の選択

- 最大限の制御を行い競合を手動で処理したい場合は `NoAutomatedResolution` を使用してください
- 削除を必ず尊重したい場合は `DeleteOnConflict` を使用してください
- 最新の操作を優先させたい場合は `TimeBasedResolution` を使用してください

### リード時修復

:::info Added in `v1.18`
:::

リードの一貫性レベルが `All` または `Quorum` に設定されている場合、リードコーディネーターは複数のレプリカからレスポンスを受け取ります。これらのレスポンスが異なる場合、以下の例のようにコーディネーターは不整合の修復を試みることができます。このプロセスは「repair-on-read」または「リードリペア」と呼ばれます。

| Problem | Action |
| :- | :- |
| オブジェクトが一部のレプリカに一度も存在しなかった。 | 欠落しているレプリカへオブジェクトを伝播します。 |
| オブジェクトが古い。 | 古いレプリカ上のオブジェクトを更新します。 |
| オブジェクトが一部のレプリカで削除されている。 | エラーを返します。削除が失敗しているか、オブジェクトが部分的に再作成された可能性があります。 |

リードリペアは、使用されるリードおよびライトの一貫性レベルにも依存します。

| Write consistency level | Read consistency level | Action |
| :- | :- |
| `ONE` | `ALL` | Weaviate は修復を保証するためにすべてのノードを検証する必要があります。 |
| `QUORUM` | `QUORUM` または `ALL` | Weaviate は同期の問題を修正しようとします。 |
| `ALL` | - | この状況は発生しないはずです。書き込みは失敗しているはずです。 |

修復はリード時にのみ行われるため、バックグラウンドの負荷は大きくありません。ノードが不整合の状態にある間、一貫性レベル `ONE` のリード操作は古いデータを返す可能性があります。

## レプリカ移動

:::info Added in `v1.32`
:::

シャードは、シングルテナントコレクションではコレクションの一部、マルチテナントコレクションでは 1 テナント全体を表します。 Weaviate では、ユーザーが Weaviate クラスター内で特定のシャードレプリカをソースノードからデスティネーションノードへ手動で移動またはコピーできます。この機能は、スケール後のクラスターリバランス、ノードの廃止、パフォーマンス向上のためのデータローカリティ最適化、または可用性向上といった運用シナリオに対応します。

レプリカ移動は、プロセス全体でデータ整合性を維持するステートマシンとして動作します。この機能はシングルテナントコレクションとマルチテナントコレクションの両方で利用可能です。

コレクション作成時に設定する静的なレプリケーションファクターとは異なり、レプリカ移動により、クラスター内でレプリカを移動またはコピーする際に特定のシャードのレプリケーションファクターを調整できます。コピー操作を行うと、新しく作成されたレプリカによってそのシャードのレプリケーションファクターが 1 増加します。コレクション全体にはデフォルトのレプリケーションファクターが設定されている場合でも、個々のシャードはそれより高いレプリケーションファクターを持つことができます。ただし、シャードがコレクションレベルで設定された値より低いレプリケーションファクターになることはできません。 

:::info

[`REPLICATION_ENGINE_MAX_WORKERS` 環境変数](/docs/deploy/configuration/env-vars/index.md#REPLICATION_ENGINE_MAX_WORKERS) を使用して、レプリカ移動を並列処理するワーカー数を調整できます。 

:::

### 移動ステート

各レプリカ移動操作は、データ整合性と可用性を維持するために次のステートを順に進みます。

- **REGISTERED**: 移動操作が開始され、Raft リーダーによってログに記録されました。リクエストが受信され、キューに登録されています。  
- **HYDRATING**: デスティネーションノードで新しいレプリカを作成中です。既存レプリカ（通常はソースレプリカ、または別の利用可能なピア）からデータセグメントが転送され、新しいレプリカを構築します。  
- **FINALIZING**: 大量データの転送が完了し、転送中に発生した書き込みを新しいレプリカが追いついています。これによりレプリカが最新データと完全に同期されます。進行中の書き込みが完了し、ターゲットノードへレプリケートされるのを保証する待機時間は [`REPLICA_MOVEMENT_MINIMUM_ASYNC_WAIT` 環境変数](/docs/deploy/configuration/env-vars/index.md#REPLICA_MOVEMENT_MINIMUM_ASYNC_WAIT) で調整できます。  
- **DEHYDRATING**: Move 操作の場合、新しいレプリカが準備完了後、ソースノード上の元のレプリカが削除されています。  
- **READY**: 操作が正常に完了しました。新しいレプリカは完全に同期され、トラフィックを処理する準備ができています。Move 操作ではソースレプリカが削除されています。  
- **CANCELLED**: 操作が完了前にキャンセルされました。手動による介入、または回復不能なエラーが発生した場合に起こります。  

レプリカ移動は 2 つの操作モードをサポートします。

- **Move**: レプリカをあるノードから別のノードへ移動し、レプリケーションファクターは維持します  
- **Copy**: レプリカをあるノードから別のノードへコピーし、そのシャードのレプリケーションファクターを 1 増加させます  

:::note Replication factor and quorum

シャードレプリカをコピーすると、レプリケーションファクターが偶数になる場合があります。この場合、クォーラムを達成するのが難しくなり、必要なノード数が `(n/2 + 1)` ではなく `(n/2 + 0.5)` になります。例えば、`RF=3` から `RF=4` になると、クォーラムに必要なノード数は 2 から 3 へ（レプリカの 67% から 75%）増えます。

:::

## 関連ページ
- [API リファレンス | GraphQL | Get | 一貫性レベル](../../api/graphql/get.md#consistency-levels)
- <SkipLink href="/weaviate/api/rest#tag/objects">API リファレンス | REST | オブジェクト</SkipLink>

## 質問とフィードバック

import DocsFeedback from '/_includes/docs-feedback.mdx';

<DocsFeedback/>